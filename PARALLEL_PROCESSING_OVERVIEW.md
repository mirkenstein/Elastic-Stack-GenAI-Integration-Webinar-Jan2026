# Parallel Logstash Pipeline for Medicare Provider Services

**Efficient, Scalable Data Ingestion from PostgreSQL to Elasticsearch**

> **ðŸ“Š Demo Dataset**: [CMS Medicare Physician & Other Practitioners by Provider and Service](https://data.cms.gov/provider-summary-by-type-of-service/medicare-physician-other-practitioners/medicare-physician-other-practitioners-by-provider-and-service)
>
> This demonstration uses publicly available, de-identified Medicare provider service data for educational purposes.

---

## Slide 1: The Challenge & Solution

### The Challenge
- **9 million+ records** per year in the [CMS Medicare Provider Services](https://data.cms.gov/provider-summary-by-type-of-service/medicare-physician-other-practitioners/medicare-physician-other-practitioners-by-provider-and-service) dataset
- Traditional single-pipeline ingestion is slow and resource-intensive
- Need for incremental updates and progress tracking
- Production system requires minimal downtime

### Our Solution: Horizontal Scaling with PK Range Partitioning
- **Multiple Logstash pods** process data in parallel
- Each pod handles a **distinct primary key (PK) range**
- Independent progress tracking per pod
- Linear scalability: Add more pods = faster ingestion

### Key Results
- âœ… **4x faster** ingestion with 4 parallel pods
- âœ… **Incremental updates** using last processed PK
- âœ… **Fault tolerant**: Pod failures don't affect other ranges
- âœ… **Resource efficient**: Distribute memory and CPU load

---

## Slide 2: Parallel Processing Architecture

### High-Level Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PostgreSQL Database                          â”‚
â”‚              cms.provider_services (9M+ records)                â”‚
â”‚                Auto-increment PK: 0 to 9,000,000               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ JDBC Queries (Parallel)
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Kubernetes Pod Distribution          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚              â”‚              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Pod 0       â”‚  â”‚  Pod 1      â”‚  â”‚  Pod 2        â”‚
        â”‚              â”‚  â”‚             â”‚  â”‚               â”‚
        â”‚ PK Range:    â”‚  â”‚ PK Range:   â”‚  â”‚ PK Range:     â”‚
        â”‚ 0 - 3M       â”‚  â”‚ 3M - 6M     â”‚  â”‚ 6M - 9M       â”‚
        â”‚              â”‚  â”‚             â”‚  â”‚               â”‚
        â”‚ Processes:   â”‚  â”‚ Processes:  â”‚  â”‚ Processes:    â”‚
        â”‚ â€¢ Fetch rows â”‚  â”‚ â€¢ Fetch rowsâ”‚  â”‚ â€¢ Fetch rows  â”‚
        â”‚ â€¢ Transform  â”‚  â”‚ â€¢ Transform â”‚  â”‚ â€¢ Transform   â”‚
        â”‚ â€¢ Enrich     â”‚  â”‚ â€¢ Enrich    â”‚  â”‚ â€¢ Enrich      â”‚
        â”‚ â€¢ Track PK   â”‚  â”‚ â€¢ Track PK  â”‚  â”‚ â€¢ Track PK    â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                 â”‚                â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ Bulk Index
                                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           Elasticsearch Cluster             â”‚
        â”‚  medicare-provider-services-2023 index      â”‚
        â”‚                                             â”‚
        â”‚  â€¢ Composite document IDs prevent dupes    â”‚
        â”‚  â€¢ Ingestion metadata tracks pod & PK      â”‚
        â”‚  â€¢ Enriched with RVU, RBCS, CBSA data      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

**1. PK Range Partitioning**
- ConfigMap defines non-overlapping PK ranges for each pod
- InitContainer reads pod index and sets PK_MIN/PK_MAX environment variables
- SQL query filters: `WHERE pk >= $PK_MIN AND pk <= $PK_MAX`

**2. Progress Tracking**
- Each pod maintains its own `.logstash_jdbc_last_run` file
- Stores the highest PK value processed
- Enables incremental updates: Only fetch rows with `pk > last_run_value`

**3. Deduplication Strategy**
- Document ID = `provider_services_rndrng_npi_hcpcs_cd_place_of_srvc_year_key`
- Composite key ensures idempotency across all pods
- Safe to re-run: Updates overwrite existing documents

**4. Metadata Enrichment**
- `ingestion_pod_index`: Tracks which pod processed the record
- `ingestion_timestamp`: When the record was indexed
- `year`: Data year for index routing

---

## Slide 3: Implementation Details

### Pod Configuration (4 Parallel Workers)

```yaml
spec:
  count: 4  # Number of parallel Logstash pods

  initContainers:
    - name: setup-pk-env
      # Reads pod index from pod name (e.g., "...-ls-2" â†’ index=2)
      # Sets PK_MIN and PK_MAX from ConfigMap
      # Creates /pk-env/.pk_env file for Logstash
```

### PK Range Distribution Example

| Pod Index | PK Min | PK Max | Est. Records | Coverage |
|-----------|--------|--------|--------------|----------|
| Pod 0     | 0      | 2.25M  | 2.25M        | 25%      |
| Pod 1     | 2.25M  | 4.5M   | 2.25M        | 25%      |
| Pod 2     | 4.5M   | 6.75M  | 2.25M        | 25%      |
| Pod 3     | 6.75M  | 10M    | 2.25M        | 25%      |

*ConfigMap automatically generated by `scripts/calculate-pk-ranges.sh`*

### Logstash Pipeline Logic

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://..."
    statement => "
      SELECT * FROM cms.provider_services
      WHERE pk >= :sql_last_value
        AND pk >= ${PK_MIN}
        AND pk <= ${PK_MAX}
      ORDER BY pk ASC
    "
    use_column_value => true
    tracking_column => "pk"
    tracking_column_type => "numeric"
  }
}

filter {
  # Add pod tracking metadata
  mutate {
    add_field => {
      "ingestion_pod_index" => "%{[metadata][name]}"
    }
  }
}

output {
  elasticsearch {
    index => "medicare-provider-services-%{year}"
    document_id => "%{provider_services_rndrng_npi_hcpcs_cd_place_of_srvc_year_key}"
    pipeline => "hcpcs_enrichment"  # Apply RVU/RBCS/CBSA enrichments
  }
}
```

### Deployment & Monitoring

**Initial Deployment**
```bash
# Calculate optimal PK ranges based on table row count
./scripts/calculate-pk-ranges.sh 4  # For 4 pods

# Deploy all resources
kubectl apply -k k8s/provider-services/
```

**Monitor Progress**
```bash
# Watch ingestion across all pods
kubectl logs -n elk -l app=medicare-provider-services -f

# Check document count by pod
GET /medicare-provider-services-2023/_search
{
  "size": 0,
  "aggs": {
    "by_pod": {
      "terms": { "field": "ingestion_pod_index" }
    }
  }
}
```

**Performance Characteristics**
- **Throughput**: ~50,000 docs/sec per pod (4 pods = 200K docs/sec)
- **Memory**: 1-2GB per pod
- **CPU**: 1-2 cores per pod
- **Initial Load**: 9M records in ~45 minutes (4 pods)
- **Incremental**: Only new/updated records processed

---

## Key Advantages of This Architecture

### âœ… Scalability
- **Horizontal scaling**: Add more pods to process faster
- **Independent execution**: Pods don't coordinate or compete
- **Linear performance**: 2x pods â‰ˆ 2x throughput

### âœ… Reliability
- **Fault isolation**: One pod failure doesn't block others
- **Resumable**: Each pod resumes from its last processed PK
- **Idempotent**: Safe to re-run with composite document IDs

### âœ… Production Ready
- **Kubernetes native**: Uses Elastic Cloud on Kubernetes (ECK) operator
- **Resource limits**: Prevents resource exhaustion
- **Progress tracking**: Monitor per-pod completion
- **ConfigMap-driven**: Easy to adjust ranges without code changes

### âœ… Enrichment at Ingestion
- **HCPCS RVU Data**: Relative value units and payment amounts
- **RBCS Classifications**: Procedure categorization
- **CBSA/County Data**: Geographic enrichment
- **Single pipeline**: All enrichments applied during ingest

---

## Use Cases

### Historical Data Migration
- Process multiple years in parallel
- Each year â†’ separate index
- Different pod counts per year based on volume

### Incremental Updates
- Run nightly/hourly for new records
- Only processes `pk > last_run_value`
- Near real-time synchronization

### Disaster Recovery
- Re-index from backup database
- Parallel restore to minimize downtime
- Metadata tracks ingestion source/time

### A/B Testing & Re-indexing
- Test new mapping or enrichment logic
- Parallel ingestion to test index
- Compare results before cutover
