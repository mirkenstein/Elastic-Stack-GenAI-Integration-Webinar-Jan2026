# LLM-Powered ETL Pipeline - Architecture Overview

> **âš ï¸ Demo Purpose**: This architecture demonstration showcases secure patterns for healthcare data processing. Examples use publicly available datasets for educational purposes only.

## Slide 1: Secure Same-Pod Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kubernetes Job Pod (Production GPU)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Init Container: Model Loader           â”‚ â”‚
â”‚  â”‚ - Pre-loads model (e.g., llama3)       â”‚ â”‚
â”‚  â”‚ - GPU: 1x NVIDIA (A10/T4)              â”‚ â”‚
â”‚  â”‚ - Exits after model ready              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  localhost:11434      â”‚
â”‚  â”‚ Local LLM        â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Service          â”‚                   â”‚   â”‚
â”‚  â”‚                  â”‚                   â”‚   â”‚
â”‚  â”‚ GPU: 1x NVIDIA   â”‚                   â”‚   â”‚
â”‚  â”‚ RAM: 12-16Gi     â”‚                   â”‚   â”‚
â”‚  â”‚                  â”‚                   â”‚   â”‚
â”‚  â”‚ (Ollama/Modular  â”‚                   â”‚   â”‚
â”‚  â”‚  MAX GPU)        â”‚                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚
â”‚                                          â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”â”‚
â”‚  â”‚ Logstash ETL Engine                  â”‚ â”‚â”‚â”‚
â”‚  â”‚                                       â”‚ â”‚â”‚â”‚
â”‚  â”‚ CPU: 2-4 cores                        â”‚ â”‚â”‚â”‚
â”‚  â”‚ RAM: 2-4Gi                            â”‚ â”‚â”‚â”‚
â”‚  â”‚                                       â”‚ â”‚â”‚â”‚
â”‚  â”‚ - Reads input data                    â”‚ â”‚â”‚â”‚
â”‚  â”‚ - Calls localhost:11434 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚  â”‚ - Enriches data with LLM              â”‚  â”‚
â”‚  â”‚ - Outputs results                     â”‚  â”‚
â”‚  â”‚ - Auto-terminates on completion       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                              â”‚
â”‚  Total: 1 GPU + 6 CPU + 16Gi RAM            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**
- **Init Container**: Pre-loads ML model before ETL starts
- **LLM Service**: Local inference engine (no external API calls)
- **ETL Engine**: Processes data with LLM enrichment
- **Communication**: Internal localhost only

---

## Slide 2: Security & Performance Benefits

### ğŸ”’ Security Advantages

**Zero Network Exposure:**
- All communication via `localhost:<port>` within single pod
- No service mesh, no network policies needed
- LLM never exposed to cluster network
- Data never leaves the pod

**No External Dependencies:**
- No API keys or cloud LLM services
- Data processed entirely on-premise
- Complete air-gap capability
- Compliance-friendly (GDPR, HIPAA, SOC2)

**Ephemeral Execution:**
- Job runs and auto-terminates
- No persistent LLM service
- Data cleaned up after TTL expiry

### âš¡ Performance

**Production (GPU):**
- Inference: ~200ms per record
- 100 records: ~30 seconds
- Total job time: ~2 minutes

**Cost Efficiency:**
- Spot GPU instances: ~$0.50/hour
- Only runs when needed (batch jobs)
- No always-on LLM service costs

---

## Slide 3: Execution Flow

### Simple 4-Step Process

```
1. Deploy â†’ 2. Process â†’ 3. Extract â†’ 4. Cleanup
```

**1. Deploy Job**
```bash
kubectl apply -f etl-job.yaml
```

**2. Automatic Processing**
- Init container downloads model (~2 min)
- LLM service starts on localhost
- Logstash processes all records
- Job terminates on completion

**3. Extract Results**
```bash
kubectl logs job/llm-etl -c logstash > results.json
```

**4. Auto-Cleanup**
- Job auto-deletes after 1 hour (configurable TTL)
- All resources released

### Use Cases

- **Sentiment Analysis**: Customer feedback classification
- **Data Enrichment**: Adding semantic tags/categories
- **Content Moderation**: Automated content screening
- **Entity Extraction**: NLP processing at scale
- **Data Transformation**: AI-powered ETL pipelines

---

## Key Takeaways

âœ… **Secure by Design**: Localhost-only, no network exposure
âœ… **Production Ready**: GPU-accelerated for real workloads
âœ… **Flexible**: Supports multiple LLM backends (Ollama, Modular MAX)
âœ… **Cost Effective**: Batch jobs on spot instances
âœ… **Compliant**: On-premise, no data egress